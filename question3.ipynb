{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6c6024",
   "metadata": {},
   "source": [
    "Q3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db52a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", DEVICE)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_MLP = 10\n",
    "EPOCHS_CNN = 10\n",
    "RSEED = 42\n",
    "np.random.seed(RSEED)\n",
    "torch.manual_seed(RSEED)\n",
    "\n",
    "# Data transforms\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),  # [0,1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "transform_fashion = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d3202ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datasets (full MNIST; you can switch to a subset if compute-limited)\n",
    "train_mnist = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform_mnist)\n",
    "test_mnist = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform_mnist)\n",
    "\n",
    "train_fashion = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform_fashion)\n",
    "test_fashion = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform_fashion)\n",
    "\n",
    "# Optional: stratified subset helper\n",
    "def stratified_subset(dataset, n_per_class=None):\n",
    "    if n_per_class is None:\n",
    "        return dataset\n",
    "    targets = np.array([label for _, label in dataset])\n",
    "    idx_per_class = [np.where(targets == i)[0] for i in range(10)]\n",
    "    chosen = []\n",
    "    rng = np.random.default_rng(RSEED)\n",
    "    for i in range(10):\n",
    "        idxs = idx_per_class[i]\n",
    "        k = min(n_per_class, len(idxs))\n",
    "        chosen.extend(rng.choice(idxs, size=k, replace=False))\n",
    "    chosen = np.array(sorted(chosen))\n",
    "    return Subset(dataset, chosen)\n",
    "\n",
    "# Create loaders (full datasets; replace with stratified_subset(dataset, n_per_class) if needed)\n",
    "train_loader_mnist = DataLoader(train_mnist, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader_mnist  = DataLoader(test_mnist, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "train_loader_fashion = DataLoader(train_fashion, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader_fashion  = DataLoader(test_fashion, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e5558dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3.1 MLP on MNIST\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 30)\n",
    "        self.fc2 = nn.Linear(30, 20)\n",
    "        self.fc3 = nn.Linear(20, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        hidden20 = self.fc2(x)  # pre-activation; we'll apply activation next\n",
    "        x = self.relu(hidden20)\n",
    "        logits = self.fc3(x)\n",
    "        return logits, hidden20  # return both logits and 20-dim features\n",
    "\n",
    "def train_mlp(model, loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def eval_mlp(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    truth = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits, _ = model(xb)\n",
    "            pred = logits.argmax(dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            truth.extend(yb.cpu().numpy())\n",
    "    acc = accuracy_score(truth, preds)\n",
    "    f1 = f1_score(truth, preds, average='macro')\n",
    "    cm = confusion_matrix(truth, preds)\n",
    "    return acc, f1, cm, preds, truth\n",
    "\n",
    "mlp = MLP().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "# Optional: train on full MNIST or a subset\n",
    "train_mlp(mlp, train_loader_mnist, optimizer, criterion, epochs=EPOCHS_MLP)\n",
    "\n",
    "# Extract 20-dim embeddings for t-SNE\n",
    "def get_mlp_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    feats = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            _, h20 = model(xb)\n",
    "            feats.append(h20.cpu().numpy())\n",
    "            labels.append(yb.numpy())\n",
    "    return np.vstack(feats), np.concatenate(labels)\n",
    "\n",
    "train_emb, train_labels = get_mlp_embeddings(mlp, train_loader_mnist)\n",
    "test_emb, test_labels = get_mlp_embeddings(mlp, test_loader_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72b35b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t-SNE visualization for trained model\n",
    "def tsne_visualize(embeddings, labels, title, save_path):\n",
    "    tsne = TSNE(n_components=2, random_state=RSEED, perplexity=30, max_iter=1000)\n",
    "    X = tsne.fit_transform(embeddings)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    scatter = plt.scatter(X[:,0], X[:,1], c=labels, cmap='tab10', s=5, alpha=0.8)\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Digits\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"TSNE-1\")\n",
    "    plt.ylabel(\"TSNE-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "tsne_visualize(train_emb, train_labels, \"t-SNE MNIST embeddings (20-dim MLP, trained)\", \"tsne_mlp_trained.png\")\n",
    "\n",
    "# Untrained model embeddings (reinitialize or use a copy before training)\n",
    "mlp_untrained = MLP().to(DEVICE)\n",
    "# Do not train; just get embeddings from random weights\n",
    "emb_untrained, labels_untrained = get_mlp_embeddings(mlp_untrained, train_loader_mnist)\n",
    "tsne_visualize(emb_untrained, labels_untrained, \"t-SNE MNIST embeddings (20-dim MLP, untrained)\", \"tsne_mlp_untrained.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97f4eadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST MLP on Fashion-MNIST - Acc: 0.0791, F1: 0.0529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cross-domain: test trained MLP on Fashion-MNIST\n",
    "# For Fashion-MNIST, we need to adapt input shape/normalization; reuse the same MLP by downsampling Fashion images\n",
    "def prepare_fashion_loader_for_mlp(loader_fashion):\n",
    "    # Create a wrapper DataLoader that returns (28x28 grayscale) tensors compatible with MLP input\n",
    "    return loader_fashion\n",
    "\n",
    "fashion_loader_for_mlp = prepare_fashion_loader_for_mlp(test_loader_fashion)  # test on Fashion-MNIST\n",
    "# Evaluate: we need to ensure label space matches (10 classes identical)\n",
    "acc_fashion, f1_fashion, cm_fashion, preds_f, truth_f = eval_mlp(mlp, fashion_loader_for_mlp)\n",
    "print(f\"MNIST MLP on Fashion-MNIST - Acc: {acc_fashion:.4f}, F1: {f1_fashion:.4f}\")\n",
    "# Note: t-SNE on Fashion embeddings\n",
    "emb_fashion, label_fashion = get_mlp_embeddings(mlp, fashion_loader_for_mlp)\n",
    "tsne_visualize(emb_fashion, label_fashion, \"t-SNE MNIST-trained MLP on Fashion-MNIST embeddings\", \"tsne_mnist_mlp_on_fashion.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8df33be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices saved as images\n",
    "def save_cm(cm, fname):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b428b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.9684, F1-score: 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karti\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.9239, F1-score: 0.9228\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Prepare flattened data from MNIST loaders for baseline models\n",
    "def prepare_flat_data(loader):\n",
    "    X, y = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb_cpu = xb.view(xb.size(0), -1).cpu().numpy()  # flatten and convert to numpy\n",
    "        y_cpu = yb.cpu().numpy()\n",
    "        X.append(xb_cpu)\n",
    "        y.append(y_cpu)\n",
    "    return np.vstack(X), np.concatenate(y)\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train_mlp, y_train_mlp = prepare_flat_data(train_loader_mnist)\n",
    "X_test_mlp, y_test_mlp = prepare_flat_data(test_loader_mnist)\n",
    "\n",
    "# Train Random Forest on flattened MNIST data\n",
    "rf_clf = RandomForestClassifier(random_state=RSEED, n_jobs=-1)\n",
    "rf_clf.fit(X_train_mlp, y_train_mlp)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_preds = rf_clf.predict(X_test_mlp)\n",
    "rf_acc = accuracy_score(y_test_mlp, rf_preds)\n",
    "rf_f1 = f1_score(y_test_mlp, rf_preds, average='macro')\n",
    "rf_cm = confusion_matrix(y_test_mlp, rf_preds)\n",
    "print(f\"Random Forest - Accuracy: {rf_acc:.4f}, F1-score: {rf_f1:.4f}\")\n",
    "\n",
    "# Train Logistic Regression on flattened MNIST data\n",
    "lr_clf = LogisticRegression(random_state=RSEED, max_iter=200, n_jobs=-1, solver='lbfgs', multi_class='multinomial')\n",
    "lr_clf.fit(X_train_mlp, y_train_mlp)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_preds = lr_clf.predict(X_test_mlp)\n",
    "lr_acc = accuracy_score(y_test_mlp, lr_preds)\n",
    "lr_f1 = f1_score(y_test_mlp, lr_preds, average='macro')\n",
    "lr_cm = confusion_matrix(y_test_mlp, lr_preds)\n",
    "print(f\"Logistic Regression - Accuracy: {lr_acc:.4f}, F1-score: {lr_f1:.4f}\")\n",
    "\n",
    "# Save confusion matrices using your existing save_cm function\n",
    "save_cm(rf_cm, \"outputs/cm_rf.png\")\n",
    "save_cm(lr_cm, \"outputs/cm_lr.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5026ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN MNIST - Acc: 0.9850, F1: 0.9849\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3.2 CNN on MNIST\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 28x28 -> 28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 14x14\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "def train_cnn(model, loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def eval_cnn(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    truth = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            truth.extend(yb.cpu().numpy())\n",
    "    acc = accuracy_score(truth, preds)\n",
    "    f1 = f1_score(truth, preds, average='macro')\n",
    "    cm = confusion_matrix(truth, preds)\n",
    "    return acc, f1, cm, preds, truth\n",
    "\n",
    "cnn = SimpleCNN().to(DEVICE)\n",
    "optimizer_cnn = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "train_loader_mnist_for_cnn = DataLoader(train_mnist, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader_mnist_for_cnn = DataLoader(test_mnist, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "train_cnn(cnn, train_loader_mnist_for_cnn, optimizer_cnn, criterion_cnn, epochs=EPOCHS_CNN)\n",
    "acc_mnist_cnn, f1_mnist_cnn, cm_mnist_cnn, preds_cnn, truth_cnn = eval_cnn(cnn, test_loader_mnist_for_cnn)\n",
    "print(f\"CNN MNIST - Acc: {acc_mnist_cnn:.4f}, F1: {f1_mnist_cnn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9b10ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sizes - MLP params: 24380, CNN params: 804554\n",
      "MLP inference time on MNIST test: 1.0478 seconds\n",
      "CNN inference time on MNIST test: 4.7820 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pretrained CNNs for inference\n",
    "# Depending on environment, you may load lightweight pretrained models and adapt 28x28 grayscale to 3-channel if needed.\n",
    "# Example: use a pretrained SmallVGG-like or MobileNet; here is a placeholder approach:\n",
    "# - If using PyTorch, you can upsample grayscale to 3 channels and resize to 224x224, then run through pretrained model.\n",
    "# For portability, you can skip actual pretrained inference if not available.\n",
    "# Below is a safe scaffold that you can fill in with actual pretrained model loading.\n",
    "\n",
    "def infer_pretrained(imgs, model):\n",
    "    # imgs: tensor [N,1,28,28], convert to required input for model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Implement according to model input requirements\n",
    "        # Return logits or probabilities\n",
    "        pass\n",
    "\n",
    "# Example of parameter counts\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "mlp_params = count_parameters(mlp)\n",
    "cnn_params = count_parameters(cnn)\n",
    "print(f\"Model sizes - MLP params: {mlp_params}, CNN params: {cnn_params}\")\n",
    "\n",
    "# Inference timing on MNIST test set for all three models\n",
    "def measure_inference_time(model, loader):\n",
    "    model.eval()\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            _ = model(xb)\n",
    "    t1 = time.time()\n",
    "    return (t1 - t0)\n",
    "\n",
    "# Time for MLP inference on MNIST test\n",
    "time_mlp_infer = measure_inference_time(mlp, test_loader_mnist)\n",
    "print(f\"MLP inference time on MNIST test: {time_mlp_infer:.4f} seconds\")\n",
    "\n",
    "# Time for CNN inference on MNIST test\n",
    "def cnn_forward_time(model, loader):\n",
    "    model.eval()\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            _ = model(xb)\n",
    "    t1 = time.time()\n",
    "    return t1 - t0\n",
    "\n",
    "time_cnn_infer = cnn_forward_time(cnn, test_loader_mnist_for_cnn)\n",
    "print(f\"CNN inference time on MNIST test: {time_cnn_infer:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d359da09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done. Results saved to outputs/ directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Confidence, results storage\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "np.save(\"outputs/mlp_embeddings_train.npy\", train_emb)\n",
    "np.save(\"outputs/mlp_embeddings_test.npy\", test_emb)\n",
    "np.save(\"outputs/mlp_train_labels.npy\", train_labels)\n",
    "np.save(\"outputs/mlp_test_labels.npy\", test_labels)\n",
    "np.save(\"outputs/fashion_embeddings.npy\", emb_fashion)\n",
    "\n",
    "# Confusion matrices saved as images\n",
    "def save_cm(cm, fname):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    plt.close()\n",
    "\n",
    "save_cm(cm_mnist_cnn, \"outputs/cm_mnist_cnn.png\")\n",
    "save_cm(cm_mnist_cnn, \"outputs/cm_mlp_and_cnn.png\")  # reuse for consistency\n",
    "save_cm(cm_fashion, \"outputs/cm_fashion.png\")  # if you compute it later\n",
    "\n",
    "print(\"All done. Results saved to outputs/ directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df8cde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "# from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# # Use safe test loader with num_workers=0 to avoid multiprocessing issues\n",
    "# test_loader_mnist_safe = DataLoader(test_mnist, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# # Load pretrained MobileNetV2 and adjust final layer for 10 classes\n",
    "# mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "# mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 10)\n",
    "# mobilenet = mobilenet.to(DEVICE).eval()\n",
    "\n",
    "# # MobileNetV2 inference\n",
    "# preds_mobilenet = []\n",
    "# labels_mobilenet = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for xb, yb in test_loader_mnist_safe:\n",
    "#         xb = nn.functional.interpolate(xb, size=(224, 224), mode='bilinear')\n",
    "#         if xb.shape[1] == 1:\n",
    "#             xb = xb.repeat(1, 3, 1, 1)  # convert grayscale to RGB\n",
    "#         xb = xb.to(DEVICE)\n",
    "#         logits = mobilenet(xb)\n",
    "#         preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "#         preds_mobilenet.extend(preds)\n",
    "#         labels_mobilenet.extend(yb.numpy())\n",
    "\n",
    "# mobilenet_acc = accuracy_score(labels_mobilenet, preds_mobilenet)\n",
    "# mobilenet_f1 = f1_score(labels_mobilenet, preds_mobilenet, average='macro')\n",
    "# mobilenet_cm = confusion_matrix(labels_mobilenet, preds_mobilenet)\n",
    "# print(f\"MobileNetV2 - Acc: {mobilenet_acc:.4f}, F1: {mobilenet_f1:.4f}\")\n",
    "\n",
    "# # Load pretrained EfficientNet and adjust final layer for 10 classes\n",
    "# efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "# efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, 10)\n",
    "# efficientnet = efficientnet.to(DEVICE).eval()\n",
    "\n",
    "# # EfficientNet inference\n",
    "# preds_effnet = []\n",
    "# labels_effnet = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for xb, yb in test_loader_mnist_safe:\n",
    "#         xb = nn.functional.interpolate(xb, size=(224, 224), mode='bilinear')\n",
    "#         if xb.shape[1] == 1:\n",
    "#             xb = xb.repeat(1, 3, 1, 1)\n",
    "#         xb = xb.to(DEVICE)\n",
    "#         logits = efficientnet(xb)\n",
    "#         preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "#         preds_effnet.extend(preds)\n",
    "#         labels_effnet.extend(yb.numpy())\n",
    "\n",
    "# effnet_acc = accuracy_score(labels_effnet, preds_effnet)\n",
    "# effnet_f1 = f1_score(labels_effnet, preds_effnet, average='macro')\n",
    "# effnet_cm = confusion_matrix(labels_effnet, preds_effnet)\n",
    "# print(f\"EfficientNet - Acc: {effnet_acc:.4f}, F1: {effnet_f1:.4f}\")\n",
    "\n",
    "# # Save confusion matrices\n",
    "# save_cm(mobilenet_cm, \"outputs/cm_mobilenet.png\")\n",
    "# save_cm(effnet_cm, \"outputs/cm_efficientnet.png\")\n",
    "\n",
    "# # Model size (parameter count)\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# print(f\"MobileNetV2 params: {count_parameters(mobilenet)}\")\n",
    "# print(f\"EfficientNet params: {count_parameters(efficientnet)}\")\n",
    "\n",
    "# # Inference time measurement\n",
    "# import time\n",
    "\n",
    "# def measure_inference_time(model, loader):\n",
    "#     model.eval()\n",
    "#     t0 = time.time()\n",
    "#     with torch.no_grad():\n",
    "#         for xb, _ in loader:\n",
    "#             xb = nn.functional.interpolate(xb, size=(224, 224), mode='bilinear')\n",
    "#             if xb.shape[1] == 1:\n",
    "#                 xb = xb.repeat(1, 3, 1, 1)\n",
    "#             xb = xb.to(DEVICE)\n",
    "#             _ = model(xb)\n",
    "#     t1 = time.time()\n",
    "#     return t1 - t0\n",
    "\n",
    "# mobilenet_time = measure_inference_time(mobilenet, test_loader_mnist_safe)\n",
    "# effnet_time = measure_inference_time(efficientnet, test_loader_mnist_safe)\n",
    "# print(f\"MobileNetV2 inference time: {mobilenet_time:.4f} seconds\")\n",
    "# print(f\"EfficientNet inference time: {effnet_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28218072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karti\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karti\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\karti\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 - Acc: 0.9655, F1: 0.9652\n",
      "EfficientNet - Acc: 0.9631, F1: 0.9628\n",
      "MobileNetV2 params: 2236682\n",
      "EfficientNet params: 4020358\n",
      "MobileNetV2 inference time: 11.6316 seconds\n",
      "EfficientNet inference time: 11.8338 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import time\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "LR = 0.001\n",
    "\n",
    "# Data transforms to match ImageNet-pretrained models\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Converts MNIST to 3 channel\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load MNIST datasets\n",
    "train_mnist = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_mnist = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_mnist, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_mnist, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# MobileNetV2 setup\n",
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 10)\n",
    "mobilenet = mobilenet.to(DEVICE)\n",
    "\n",
    "# EfficientNet setup\n",
    "efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "efficientnet.classifier[1] = nn.Linear(efficientnet.classifier[1].in_features, 10)\n",
    "efficientnet = efficientnet.to(DEVICE)\n",
    "\n",
    "# Freeze backbone parameters if you only want to train classifier:\n",
    "for param in mobilenet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in efficientnet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Setup optimizer and loss\n",
    "optimizer_mobilenet = torch.optim.Adam(mobilenet.classifier.parameters(), lr=LR)\n",
    "optimizer_efficientnet = torch.optim.Adam(efficientnet.classifier.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop function\n",
    "def train_model(model, optimizer, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Train both models\n",
    "train_model(mobilenet, optimizer_mobilenet, train_loader, NUM_EPOCHS)\n",
    "train_model(efficientnet, optimizer_efficientnet, train_loader, NUM_EPOCHS)\n",
    "\n",
    "# Evaluation/inference\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            labels.extend(yb.numpy())\n",
    "    return preds, labels\n",
    "\n",
    "preds_mobilenet, labels_mobilenet = evaluate_model(mobilenet, test_loader)\n",
    "mobilenet_acc = accuracy_score(labels_mobilenet, preds_mobilenet)\n",
    "mobilenet_f1 = f1_score(labels_mobilenet, preds_mobilenet, average='macro')\n",
    "mobilenet_cm = confusion_matrix(labels_mobilenet, preds_mobilenet)\n",
    "print(f\"MobileNetV2 - Acc: {mobilenet_acc:.4f}, F1: {mobilenet_f1:.4f}\")\n",
    "\n",
    "preds_effnet, labels_effnet = evaluate_model(efficientnet, test_loader)\n",
    "effnet_acc = accuracy_score(labels_effnet, preds_effnet)\n",
    "effnet_f1 = f1_score(labels_effnet, preds_effnet, average='macro')\n",
    "effnet_cm = confusion_matrix(labels_effnet, preds_effnet)\n",
    "print(f\"EfficientNet - Acc: {effnet_acc:.4f}, F1: {effnet_f1:.4f}\")\n",
    "\n",
    "# Model size (parameter count)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "print(f\"MobileNetV2 params: {count_parameters(mobilenet)}\")\n",
    "print(f\"EfficientNet params: {count_parameters(efficientnet)}\")\n",
    "\n",
    "# Inference time measurement\n",
    "def measure_inference_time(model, loader):\n",
    "    model.eval()\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            _ = model(xb)\n",
    "    t1 = time.time()\n",
    "    return t1 - t0\n",
    "mobilenet_time = measure_inference_time(mobilenet, test_loader)\n",
    "effnet_time = measure_inference_time(efficientnet, test_loader)\n",
    "print(f\"MobileNetV2 inference time: {mobilenet_time:.4f} seconds\")\n",
    "print(f\"EfficientNet inference time: {effnet_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
